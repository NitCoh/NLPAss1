{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3+"
    },
    "colab": {
      "name": "part1.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWh_5rVkOyMl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import justext\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import collections"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmdROlJJNtLk",
        "colab_type": "code",
        "outputId": "27725f49-aca8-4139-d467-98ac00cae865",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!pip install justext"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting justext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/5f/c7b909b4b864ebcacfac23ce2f6f01a50c53628787cc14b3c06f79464cab/jusText-2.2.0-py2.py3-none-any.whl (860kB)\n",
            "\r\u001b[K     |▍                               | 10kB 20.4MB/s eta 0:00:01\r\u001b[K     |▊                               | 20kB 3.2MB/s eta 0:00:01\r\u001b[K     |█▏                              | 30kB 4.6MB/s eta 0:00:01\r\u001b[K     |█▌                              | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |██                              | 51kB 3.7MB/s eta 0:00:01\r\u001b[K     |██▎                             | 61kB 4.4MB/s eta 0:00:01\r\u001b[K     |██▋                             | 71kB 5.1MB/s eta 0:00:01\r\u001b[K     |███                             | 81kB 5.7MB/s eta 0:00:01\r\u001b[K     |███▍                            | 92kB 6.4MB/s eta 0:00:01\r\u001b[K     |███▉                            | 102kB 5.0MB/s eta 0:00:01\r\u001b[K     |████▏                           | 112kB 5.0MB/s eta 0:00:01\r\u001b[K     |████▋                           | 122kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 133kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 143kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 153kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 163kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 174kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 184kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 194kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 204kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 215kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 225kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 235kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 245kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 256kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 266kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 276kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 286kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 296kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 307kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 317kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 327kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 337kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 348kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 358kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 368kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 378kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 389kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 399kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 409kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 419kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 430kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 440kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 450kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 460kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 471kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 481kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 491kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 501kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 512kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 522kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 532kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 542kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 552kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 563kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 573kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 583kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 593kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 604kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 614kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 624kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 634kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 645kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 655kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 665kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 675kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 686kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 696kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 706kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 716kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 727kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 737kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 747kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 757kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 768kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 778kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 788kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 798kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 808kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 819kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 829kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 839kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 849kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 860kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 870kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from justext) (4.2.6)\n",
            "Installing collected packages: justext\n",
            "Successfully installed justext-2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ8c3aHnlDx5",
        "colab_type": "code",
        "outputId": "d5de1deb-bd0a-4032-e8dd-51f238981e88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bo6kW-6tyCPg",
        "colab_type": "text"
      },
      "source": [
        "1.1.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eDGnqmpRq9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def top_txt(txt,top):\n",
        "  most_common_words = [word for (word,f) in collections.Counter(txt).most_common(top)]\n",
        "  return ''.join([word if word in most_common_words or word=='N' else '<unk>' for word in txt])\n",
        "def replace_numbers(txt):\n",
        "  #TODO:: Check replace numbers RegEx.\n",
        "  num_format = re.compile(\"^[\\-]?[1-9][0-9]*\\.?[0-9]+$\")\n",
        "  return \"\".join([word if not re.match(num_format,word) else 'N' for word in txt])\n",
        "def remove_punc(txt):\n",
        "  without_punc = \"\".join([ c if c not in '.,:;?!@#*&%$<>' else '' for c in txt])\n",
        "  return without_punc.replace(\"  \",\" \")\n",
        "def tokenize(txt):\n",
        "  return \" \".join(nltk.word_tokenize(txt))\n",
        "def segment_sents(txt):\n",
        "  return \"\\n\".join(nltk.sent_tokenize(txt))\n",
        "def lower_txt(txt):\n",
        "  return txt.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2KptyAiOyM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ptb_preprocess(filenames, top=10000):\n",
        "  for filename in filenames:\n",
        "    with open(filename) as myfile:\n",
        "      \n",
        "\n",
        "      txt_file =  myfile.read().replace('\\n',' ')\n",
        "      paragraphs = justext.justext(txt_file, justext.get_stoplist(\"English\"))\n",
        "      raw_text = \"\\n\".join([p.text for p in paragraphs if not p.is_boilerplate])\n",
        "\n",
        "\n",
        "      # raw_text = lower_txt(raw_text)\n",
        "      # raw_text = segment_sents(raw_text)\n",
        "      # print('segment_sents')\n",
        "      # print(raw_text[:200])\n",
        "      # raw_text = remove_punc(raw_text)\n",
        "      # print('remove_punkt')\n",
        "      # print(raw_text[:200])\n",
        "      # raw_text = tokenize(raw_text)\n",
        "      # print('tokenize')\n",
        "      # print(raw_text[:200])\n",
        "      # raw_text = replace_numbers(raw_text)\n",
        "      # print('replace_numbers')\n",
        "      # print(raw_text[:200])\n",
        "      # raw_text = top_txt(raw_text,top)\n",
        "      # print('top_txt')\n",
        "      # print(raw_text[:200])\n",
        "\n",
        "      raw_text = top_txt(raw_text,top)\n",
        "      raw_text = replace_numbers(raw_text)\n",
        "      raw_text = tokenize(raw_text)\n",
        "      raw_text = segment_sents(raw_text)\n",
        "      raw_text = remove_punc(raw_text)\n",
        "      raw_text = lower_txt(raw_text)\n",
        "      \n",
        "      \n",
        "      new_filename = filename+'.out'\n",
        "      #print(new_filename)\n",
        "      with open(new_filename, \"w\") as text_file:\n",
        "        text_file.write(raw_text)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNWfFRBhQCz2",
        "colab_type": "code",
        "outputId": "abbd5ffb-16d9-483a-84ac-8961b12b240b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-16 17:10:34--  https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\n",
            "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4573338 (4.4M) [text/plain]\n",
            "Saving to: ‘shakespeare_input.txt’\n",
            "\n",
            "\rshakespeare_input.t   0%[                    ]       0  --.-KB/s               \rshakespeare_input.t  66%[============>       ]   2.88M  14.4MB/s               \rshakespeare_input.t 100%[===================>]   4.36M  14.5MB/s    in 0.3s    \n",
            "\n",
            "2019-12-16 17:10:34 (14.5 MB/s) - ‘shakespeare_input.txt’ saved [4573338/4573338]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts1OWn9yE5hx",
        "colab_type": "text"
      },
      "source": [
        "1.1.1 Discussion: According to the descripted above if we apply the conventions of Penn Treebank, we shall give independent meaning for every word after the tokenization. Also, if we use character-level language model, we shall get another definition - morphological, because now we care about structure of the word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1twpt96H3Ja",
        "colab_type": "text"
      },
      "source": [
        "1.1.2 :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl1uQDDlH55T",
        "colab_type": "code",
        "outputId": "65635820-5d18-40cd-bc20-566a7c5174d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
        "!tar zxvf simple-examples.tgz\n",
        "\n",
        "!ls simple-examples/data\n",
        "!mv ./simple-examples/data ../data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-15 20:00:20--  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
            "Resolving www.fit.vutbr.cz (www.fit.vutbr.cz)... 147.229.9.23, 2001:67c:1220:809::93e5:917\n",
            "Connecting to www.fit.vutbr.cz (www.fit.vutbr.cz)|147.229.9.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 34869662 (33M) [application/x-gtar]\n",
            "Saving to: ‘simple-examples.tgz’\n",
            "\n",
            "simple-examples.tgz 100%[===================>]  33.25M  1.70MB/s    in 23s     \n",
            "\n",
            "2019-12-15 20:00:45 (1.46 MB/s) - ‘simple-examples.tgz’ saved [34869662/34869662]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VgRPhj407Tc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf00c65b-1f18-49fa-98ad-5cfc4cfc5d9d"
      },
      "source": [
        ""
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'as'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_9519iByOPy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "d5045757-9e32-43df-ab25-a6b816814b64"
      },
      "source": [
        "def num_token(txt):\n",
        "  txt = txt.lower()\n",
        "  return len(nltk.word_tokenize(txt))\n",
        "def num_char(txt):\n",
        "  return len(''.join(txt.split()))\n",
        "def dist_words(txt): #num of types\n",
        "  txt = txt.lower()\n",
        "  len(set(remove_punc(txt).split()))\n",
        "def topN_words(txt,N):\n",
        "  most_common_words = ' '.join([word for (word,f) in collections.Counter(txt).most_common(N)])\n",
        "  return num_tokenize(most_common_words)\n",
        "def token_type_ratio(txt):\n",
        "  return num_token(txt)/dist_words(txt)\n",
        "def types_oov(txt): #Out Of Vocabulary \n",
        "\n",
        "def avg_sd_char_token(txt):\n",
        "  total_chars = sum( [len(w) for w in nltk.tokenize(txt)])\n",
        "  avg =  total_chars/num_token(txt)\n",
        "\n",
        "  total_avg_chars = sum( [len(w)-avg for w in nltk.tokenize(txt)])\n",
        "  sd =  total_chars/num_token(txt)\n",
        "  return avg,sd\n",
        "def dist_n_gram_words(txt,n=[2,3,4]):\n",
        "  txt = txt.lower()\n",
        "  txt = txt.split() #maybe we should use nltk.tokenize()\n",
        "  ans = {}\n",
        "  for i in n:\n",
        "    ans[i] = diff_grams(txt,n)\n",
        "  return ans\n",
        "\n",
        "def dist_n_gram_chars(txt,n=range(1,8)):\n",
        "  ans = {}\n",
        "  for i in n:\n",
        "    ans[i] = diff_grams(txt,n)\n",
        "  return ans\n",
        "\n",
        "def diff_grams(txt,n):\n",
        "  s = set()\n",
        "  gram = [0:i]\n",
        "  while gram[-1]==len(txt):\n",
        "    next_gram = ''.join([txt[g] for g in gram])\n",
        "    s.add(next_gram)\n",
        "    gram = [g+1 for g in gram]\n",
        "  return len(s)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-38-517d5db4f97b>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    def token_type_ratio(txt):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY9AIs-GKmGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "dir = '../data'\n",
        "for filename in os.listddir(dir):\n",
        "  \n",
        "  raw_text,freq_words, tokenized_words, tokenized_sents = ptb_preprocess(['shakespeare_input.txt'])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}