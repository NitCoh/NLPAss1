{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3+"
    },
    "colab": {
      "name": "part1.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWh_5rVkOyMl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import justext\n",
        "from nltk.tokenize.api import StringTokenizer as tokenizer\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ8c3aHnlDx5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "809b4e49-8d22-4a5d-c2b6-6974a4f2ccc2"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eDGnqmpRq9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def top_txt(txt,top):\n",
        "  return [word for (word,f) in Counter(txt).most_common(top)]\n",
        "\n",
        "def replace_numbers(txt):\n",
        "  [word if word.isalpha() else 'N' for word in txt]\n",
        "def tokenize_remove_punc(txt):\n",
        "  return list(filter(lambda word: word not in '.,:;?!@#*&%$<>', tokenizer.tokenize(txt)))\n",
        "def segment_sents(txt):\n",
        "  return sent_tokenize(txt)\n",
        "def lower_txt(txt):\n",
        "  return txt.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2KptyAiOyM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ptb_preprocess(filenames, top=1e4):\n",
        "  for filename in filenames:\n",
        "    words = []\n",
        "    with open(file) as myfile:\n",
        "      next_line = next(myfile)\n",
        "      for w in next_line:\n",
        "          words.append(w)\n",
        "    \n",
        "    \n",
        "    words = lower_txt(words)\n",
        "    words = top_txt(txp,top)\n",
        "    words = replace_numbers(words)\n",
        "    tokenized_words = tokenize(words)\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhQV8Zdek-V2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}